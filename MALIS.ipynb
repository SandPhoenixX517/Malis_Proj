{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLTpO6iuXvEu"
   },
   "source": [
    "### Model:\n",
    "The NN used in the paper is InceptionV1, known as GoogLeNet too. Its implementation is present and there is no need to re-implement it from scratch. In this document, we will be loading a version of it that is shared on Internet Via the pretrained models of Pytorch.<br>\n",
    "The version that we will load is trained to classify images of ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "oe6i4bdJXvEu"
   },
   "outputs": [],
   "source": [
    "#Loading of the libraries needed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np, random\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.utils.data as data\n",
    "from skimage import io, transform, img_as_float\n",
    "import PIL\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "Image_size = 256 # This one is to be changed with the image size that we want to train with: (Image_size X Image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-mVyaPAXvEv",
    "outputId": "13d510c7-28b8-4831-b5c7-2cded571a3eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Utilisateur/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anything to omit showing architecture! ;p\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'googlenet', pretrained=True)\n",
    "in_features = model.fc.in_features\n",
    "# In the classification task, we have only 2 classes. 0 for \"no Car\" and 1 when \"at least a car\".\n",
    "out_features = 2\n",
    "model.fc = nn.Linear(in_features,out_features)\n",
    "model.to(device)\n",
    "print(\"Anything to omit showing architecture! ;p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnRQSqQ-auSX"
   },
   "source": [
    "### Preparing necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "yxUs4PcMazQn"
   },
   "outputs": [],
   "source": [
    "#Definition of train and test functions!\n",
    "def train(model, trainloader,epochs=2, lr=LEARNING_RATE):\n",
    "    total_loss = 0\n",
    "    # We need to verify whether we want to modify the Loss function. I believe we do but I am not sure yet.\n",
    "    loss_function=nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        total_loss = 0\n",
    "        total_correct=0\n",
    "        for batch_idx, (images, labels) in enumerate(trainloader):\n",
    "            images=images.to(device)\n",
    "            labels=labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            preds= model(images)\n",
    "            loss = loss_function(preds,labels)\n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            total_loss += loss.item()\n",
    "    return model\n",
    "def test(model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "du3CYje0cdtP"
   },
   "outputs": [],
   "source": [
    "# Definition of DataSet creator, This needs to be revisited because, I do not have the format of the .txt file\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, number_images, root_dir, folder, label_file , transform=None):\n",
    "        # Number of images to be loaded.\n",
    "        self.number_images = number_images\n",
    "        with open(root_dir+label_file, 'r') as file:\n",
    "            lines=file.read().splitlines()\n",
    "        self.Names_labels = []\n",
    "        for element in lines:\n",
    "            self.Names_labels.append(element.split(' '))\n",
    "        \n",
    "        # Here I construct the path to the images\n",
    "        self.root_dir = root_dir+folder\n",
    "        # this is to collect the transform, if I supply any.\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.number_images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = os.path.join(self.root_dir,self.Names_labels[idx][0].split('/')[-1])\n",
    "        image = io.imread(img_name)\n",
    "        resized_img = transform.resize(image, (Image_size, Image_size))\n",
    "        sample = img_as_float(resized_img)\n",
    "        target = int(float(self.Names_labels[idx][1]))\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return [sample.float(), target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3v5oW1-4XvEw"
   },
   "source": [
    "### Loading Dataset\n",
    "In this phase, we will only consider using the portion of dataset captured from Colombus city. The whole dataset will be used when everything is set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "KJMK4S6xetA-"
   },
   "outputs": [],
   "source": [
    "Path_train =\".\"\n",
    "number_images_training = 6966 \n",
    "transformation = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "train_dataset = Dataset(number_images=number_images_training, root_dir=Path_train, folder = '/train',\n",
    "                        label_file = '/COWC_train_list_detection.txt', transform = transformation)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "Path_test = \".\"\n",
    "number_images_testing = 2050\n",
    "test_dataset = Dataset(number_images=number_images_testing, root_dir=Path_test, folder= '/test',\n",
    "                       label_file = '/COWC_test_list_detection.txt', transform = transformation)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3SzBVwcXvEw"
   },
   "source": [
    "### Training for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "-XQC9-j_gSmH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [04:15<17:03, 255.91s/it]\u001b[A\n",
      " 40%|████      | 2/5 [08:52<13:06, 262.10s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [13:36<08:57, 268.56s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [18:20<04:33, 273.34s/it]\u001b[A\n",
      "100%|██████████| 5/5 [23:07<00:00, 277.57s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Normally if all the points above that needs to be filled are done, we can start the training without any further processing.\n",
    "model = train(model, train_dataloader,epochs=5, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.26829268292683"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qK1BXuxrXvEw"
   },
   "source": [
    "### Training for Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FomRs_5FXvEw"
   },
   "source": [
    "### Training for Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "kELEBX7mXvEw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MALIS_Njeh .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
